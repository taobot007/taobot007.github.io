---
layout: post
title: Scorecard 模型实践
date: 2019-07-17
tag: Big Data and Machine Learning
---

Scorecard Model:


WOE(Weight of Evidence): 证据权重转换可以将logistic回归模型转变为标准评分卡格式。该转换过程能够简化模型的应用且增加业务解释性，同时能将特征与标签之间的非线性关系转化为线性的。

scorecard::woebin函数tree方法分箱的原理类似决策树过程。 不同的变量类型的分箱过程稍有差异.

scorecard::woebin函数chimerge分箱为自底向上的数据离散化方法，即将具有最小卡方值的相邻区间合并在一起。具体内容参见chimerge算法或原论文ChiMerge:Discretization of numeric attributs.

问题：
1. Monotonicity Constraint, 和WOE什么关系？
2. 怎样分箱， binning？ Chimerge？
3. 拒绝推断： 申请评分卡的模型开发过程中使用的数据实际上并不是从申请总体样本中随机选择的，而仅仅是从过去已经被接受的客户样本中选择的。因此，开发申请评分卡时将对被拒绝客户的状态进行推断并纳入模型开发数据集中，即拒绝推断过程。

拒绝推断的常用方法包括，

简单赋值法：人为指定被拒绝账户的标签
    忽略被拒绝申请
    所有被拒申请赋值为违约标签
    按比例赋值，使得其坏客户率是通过样本的2~5倍以上
强化法：通过外推法确定拒绝账户的标签
    简单强化法：使用通过客户开发的模型对被拒绝客户评分，将其中低分段赋予违约标签。使得拒绝客户的坏客户率为通过的2~5倍以上
    模糊强化法：通过模型计算得到正常和违约概率。
    打包强化法：先用开发的评分卡对被拒客户评分，然后指定每个分值区间的违约客户数量。
4. 模型评估指标： KS？ AUC？ Gini？ROC?
5. 稳定性指数？ PSI？
6. L1, L2 norm regulation?
7. 单变量分析： univariate analysis


## 前提知识：

* ChiMerge: 用来合并bin，而要了解chi-merge，就要了解chi-square test

* KS score: 用来查看单边量对于y值的重要程度


### chi-square test

chi-square test 卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。

chi-square test 步骤：

1.  提出原假设：

    H0：总体X的分布函数为F(x).

    如果总体分布为离散型，则假设具体为 H0：总体X的分布律为P{X=xi}=pi， i=1，2，...

2.  将总体X的取值范围分成k个互不相交的小区间A1，A2，A3，…，Ak，如可取

    A1=（a0，a1]，A2=(a1，a2]，...，Ak=(ak-1,ak)，

    其中a0可取-∞，ak可取+∞，区间的划分视具体情况而定，但要使每个小区间所含的样本值个数不小于5，而区间个数k不要太大也不要太小。

3.  把落入第i个小区间的Ai的样本值的个数记作fi，成为组频数（真实值），所有组频数之和f1+f2+...+fk等于样本容量N。

4.  当H0为真时，根据所假设的总体理论分布，可算出总体X的值落入第i 个小区间Ai的概率pi，于是，N*pi就是落入第i个小区间Ai的样本值的理论频数（理论值）。

5.  当H0为真时，N次试验中样本值落入第i个小区间Ai的频率fi/N与概率pi应很接近，当H0不真时，则fi/N与pi相差很大。基于这种思想，皮尔逊引进如下检验统计量

     $x^2 = \sum_{i=1}^{k}  \frac{ (f_i - N * p_i)^2 }{N * p_i}$

    ，在0假设成立的情况下服从自由度为k-1的卡方分布。

chi-sqare test 的应用：

判断两个变量X与Y到底有没有关系。

假设有两个分类变量X和Y，它们的值域分别为{x1, x2}和{y1, y2}，其样本频数列联表为

| ---- | y1 | y2 | 总计 |
| x1 | a | b | a + b |
| x2 | c | d | c + d |
| 总计 | a + c | b + d | a + b + c + d |

若要推断的论述为H1：“X与Y有关系”，可以利用独立性检验来考察两个变量是否有关系，并且能较精确地给出这种判断的可靠程度。具体的做法是，由表中的数据算出检验统计量$x^2$的值。这个值越大，说明“X与Y有关系”成立的可能性越大。

具体例子：

H0: 性别与化妆与否没有关系。

然后我们看如下表：

| ---- | man | woman | total |
| makeup | 15 (55) | 95 (55) | 110 |
| no makeup | 85 (45) | 5 (45) | 90 |
| ---- | 100 | 100 | 200 |

如果性别和化妆与否没有关系，四个格子应该是括号里的数（期望值，用极大似然估计55=100*110/200，其中110/200可理解为化妆的概率，乘以男人数100，得到男人化妆概率的似然估计），这和实际值（括号外的数）有差距，理论和实际的差距说明这不是随机的组合。

应用拟合度公式:

$x^2 = \sum_{i=1}^{k}  \frac{ (f_i - N * p_i)^2 }{N * p_i} = \frac{ (95 - 55 )^2 }{55} + \frac{ (15 - 55 )^2 }{55} + \frac{ (85 - 45 )^2 }{55} + \frac{ (5 - 45 )^2 }{55} = 129.3$

而我们知道当$x^2$ > 10.828时，就以为着有99.9%的可能H0是不成立的。所以我们可以知道性别与化妆是有很大关系的。

### ChiMerge

ChiMerge 是监督的、自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。


关于分箱的讨论：

    1. 最简单的离散算法是： 等宽区间。 从最小值到最大值之间,，均分为NN等份， 这样， 如果A,BA,B为最小最大值， 则每个区间的长度为W=(B−A)/NW=(B−A)/N, 则区间边界值为 A+W,A+2W,….A+(N−1)WA+W,A+2W,….A+(N−1)W.

    2. 还有一种简单算法，等频区间。区间的边界值要经过选择，使得每个区间包含大致相等的实例数量。比如说 N=10N=10，每个区间应该包含大约10%的实例。

    3. 以上两种算法有弊端：比如，等宽区间划分，划分为5区间，最高工资为50000，则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反，所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型，落在正确区间里的偶然性很大。

    4. C4、CART、PVMC4、CART、PVM算法在离散属性时会考虑类信息，但是是在算法实施的过程中间，而不是在预处理阶段。例如，C4C4算法（ID3决策树系列的一种），将数值属性离散为两个区间，而取这两个区间时，该属性的信息增益是最大的。

    5. 评价一个离散算法是否有效很难，因为不知道什么是最高效的分类。

    6. 离散化的主要目的是：消除数值属性以及为数值属性定义准确的类别。

    7. 高质量的离散化应该是：区间内一致，区间之间区分明显。

    8. ChiMergeChiMerge算法用卡方统计量来决定相邻区间是否一致或者是否区别明显。如果经过验证，类别属性独立于其中一个区间，则这个区间就要被合并。

    9. ChiMerge算法包括2部分：1、初始化，2、自底向上合并，当满足停止条件的时候，区间合并停止。

步骤

    1. 初始化

    根据要离散的属性对实例进行排序：每个实例属于一个区间

    2. 合并区间，又包括两步骤

        (1) 计算每一对相邻区间的卡方值

        (2) 将卡方值最小的一对区间合并

    预先设定一个卡方的阈值，在阈值之下的区间都合并，阈值之上的区间保持分区间。

卡方的计算公式：

![chi calculation](/assets/images/chi square equation.png)

### KS test and KS score

KS检验是比较一个频率分布f(x)与理论分布g(x)或者两个观测值分布的检验方法。其原假设H0:两个数据分布一致或者数据符合理论分布。D=max| f(x)- g(x)|，当实际观测值D>D(n,α)则拒绝H0，否则则接受H0假设。KS检验与t-检验之类的其他方法不同是KS检验不需要知道数据的分布情况，算是一种非参数检验方法。

PS：t-检验的假设是检验的数据满足正态分布，否则对于小样本不满足正态分布的数据用t-检验就会造成较大的偏差，虽然对于大样本不满足正态分布的数据而言t-检验还是相当精确有效的手段。

在金融领域中，我们的y值和预测得到的违约概率刚好是两个分布未知的两个分布。好的信用风控模型一般从准确性、稳定性和可解释性来评估模型。一般来说。好人样本的分布同坏人样本的分布应该是有很大不同的，KS正好是有效性指标中的区分能力指标：KS用于模型风险区分能力进行评估，KS指标衡量的是好坏样本累计分布之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。

KS的score计算步骤如下：

    1. 计算每个评分区间的好坏账户数（计算的是特征的KS的话，是每个特征对应的好坏账户数）。
    2. 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。
    3. 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的KS值。

### Feature Selection

I will share 3 Feature selection techniques that are easy to use and also gives good results.

1. Univariate Selection   ----> 依据 $x^2$ statistical test

2. Feature Importance     ----> 依据Decision Tree中的Gini系数或者Entropy

3. Correlation Matrix with Heatmap ----> 计算 correlation 的大小

### Domain Knowledge

1. Observation Date: 指的是开始观察对时间。所有对attribute都是在这个时间之前对信息，而tag是这个时间之后的预测，比如说这个日期之后的十二个月。

2.

## 我的实践

### 从XX team拿到数据之后，我接到的任务是“study the data”.着是个很模糊的任务。我做了一下几点：

1. 查看了datafram的shape，有多少列，多少行

2. 找到tag是哪个，id是哪个，time在哪里

3. 看看针对time的bad rate的distribution是什么.

4. 试着解释每个attribute是什么意思

5. 在这个过程中，我发现这个dataset有5000多个attribute，一个一个解读它们将是非常繁重的工作。所以我把它们归成几十个大类，聚焦打击范围

6. 而record共有2million个，对于我拥有的资源来说，数量偏大。我采用了别人的建议，抽取了百分之五的样本。这样计算就快捷多了。

7. 根据XXX的建议，我要对每个attribute计算出KS score。然后按照score从大到小排列attribute，并选取其中的几百至一千多个进行更深的分析。

8. 同时，我要对bad rate进行一个时间上对分析，看看有没有什么问题。

--------------
Reference:

https://blog.csdn.net/qunxingvip/article/details/50449376